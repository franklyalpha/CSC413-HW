{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "# Setup PyTorch\n",
        "\n",
        "All files will be stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d993b8-3e0d-4db0-e203-27ac333b703e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "/content/content/csc421/a3\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install Pillow\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(\n",
        "    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n",
        "):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "    Arguments:\n",
        "        tensor: A Tensor object.\n",
        "        cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "    Returns:\n",
        "        A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel(\"Epochs\", fontsize=16)\n",
        "    plt.ylabel(\"Loss\", fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "\n",
        "    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n",
        "    plt.savefig(plt_path)\n",
        "    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n",
        "    )\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n",
        "    )\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
        "        pkl.dump(idx_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "outputs": [],
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\"\"\"\n",
        "    lines = open(filename).read().strip().lower().split(\"\\n\")\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n",
        "    return all(c.isalpha() or c == \"-\" for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = {\n",
        "        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n",
        "    }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index[\"SOS\"] = start_token\n",
        "    char_to_index[\"EOS\"] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = {index: char for (char, index) in char_to_index.items()}\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = {\n",
        "        \"char_to_index\": char_to_index,\n",
        "        \"index_to_char\": index_to_char,\n",
        "        \"start_token\": start_token,\n",
        "        \"end_token\": end_token,\n",
        "    }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s, t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s, t))\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "outputs": [],
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
        "    return [char_to_index[char] for char in s] + [\n",
        "        end_token\n",
        "    ]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    return \" \".join(\n",
        "        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n",
        "    )\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
        "\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if isinstance(attention_weights, tuple):\n",
        "        ## transformer's attention mweights\n",
        "        attention_weights, self_attention_weights = attention_weights\n",
        "\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "\n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
        "        ax.set_xticklabels(\n",
        "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
        "        )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = (\n",
        "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
        "            )  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat(\n",
        "                [decoder_input, targets[:, 0:-1]], dim=1\n",
        "            )  # Gets decoder inputs by shifting the targets to the right\n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(\n",
        "                decoder_inputs, encoder_annotations, decoder_hidden\n",
        "            )\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss = compute_loss(\n",
        "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss = compute_loss(\n",
        "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
        "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
        "            )\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Data Stats\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
        "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = GRUEncoder(\n",
        "            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n",
        "        )\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "            opts=opts,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      encoder = AttentionEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            opts=opts,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == \"rnn\":\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == \"rnn_attention\":\n",
        "        decoder = RNNAttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            attention_type=opts.attention_type,\n",
        "        )\n",
        "    elif opts.decoder_type == \"transformer\":\n",
        "        decoder = TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      decoder = AttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #### setup checkpoint path\n",
        "    model_name = \"h{}-bs{}-{}-{}\".format(\n",
        "        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n",
        "    )\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return encoder, decoder, losses\n",
        "\n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Opts\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aROU2xZanDKq",
        "outputId": "fd0fdaf7-1a94-4085-ae76-04b4ec1bf180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_small.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n",
        "    untar=False,\n",
        ")\n",
        "\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_large.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n",
        "    untar=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Neural machine translation (NMT)\n",
        "\n",
        "In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "outputs": [],
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wic = nn.Linear(input_size, hidden_size)\n",
        "        self.Whc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
        "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
        "\n",
        "        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n",
        "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
        "\n",
        "        c_new = f * c_prev + i * c\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "\n",
        "        return h_new, c_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the `MyGRUCell` class defined in the next cell. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$3 * (H^2 + HV)$$"
      ],
      "metadata": {
        "id": "5SjPE16GotB2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DGyxqZIQzTJH"
      },
      "outputs": [],
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # Input linear layers\n",
        "        self.Wiz = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.Wir = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.Wih = nn.Linear(self.input_size, self.hidden_size)\n",
        "\n",
        "        # Hidden linear layers\n",
        "        self.Whz = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.Whr = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.Whh = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = torch.tanh(self.Wih(x) + self.Whh(h_prev * r))\n",
        "        h_new = (1 - z) * h_prev + z * g\n",
        "        return h_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "\n",
        "The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "outputs": [],
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "\n",
        "Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "XmVuXTozTPF7",
        "outputId": "d5b05150-4390-4e7b-8cb0-aff76680ded7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   8 | Train loss: 1.207 | Val loss: 1.480 | Gen: etay aryway ontionsay iway orway\n",
            "Epoch:   9 | Train loss: 1.170 | Val loss: 1.469 | Gen: etay-atetay arway ostionsay-ixtay-atew iway-ateway orway\n",
            "Exiting early from training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m    409\u001b[0m         losses = training_loop(\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\u001b[0m\n\u001b[1;32m    271\u001b[0m         train_loss = compute_loss(\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\u001b[0m\n\u001b[1;32m    207\u001b[0m             decoder_outputs, attention_weights = decoder(\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fe066f961765>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, annotations, hidden_init)\u001b[0m\n\u001b[1;32m     33\u001b[0m             ]  # Get the current time step input tokens, across the whole batch\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch_size x hidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mhiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d6ada70c8107>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h_prev)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# ------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWiz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-315155565817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_args_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrnn_encode_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_decoder_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_losses_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_args_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m translated = translate_sentence(\n",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting early from training.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'losses' referenced before assignment"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "13c0b2e3-9dee-49e5-fe1f-677488c20151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting early from training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m    409\u001b[0m         losses = training_loop(\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\u001b[0m\n\u001b[1;32m    274\u001b[0m         val_loss = compute_loss(\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mval_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\u001b[0m\n\u001b[1;32m    207\u001b[0m             decoder_outputs, attention_weights = decoder(\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fe066f961765>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, annotations, hidden_init)\u001b[0m\n\u001b[1;32m     33\u001b[0m             ]  # Get the current time step input tokens, across the whole batch\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch_size x hidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mhiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d6ada70c8107>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h_prev)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWih\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mh_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_prev\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-1b92f2861187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_args_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrnn_encode_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_decoder_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_losses_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_args_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m translated = translate_sentence(\n",
            "\u001b[0;32m<ipython-input-4-38e4be544722>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting early from training.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'losses' referenced before assignment"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qyk_9-Fwtekj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "586e08ce-0037-4625-bf9e-955518ec3598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved to: /content/content/csc421/a3/loss_plot_gru.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ad151b-7f1f-4933-d1fa-c9e0d80e4528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tflower frame fluctuation forseeable forsake \n",
            "translated:\towenseday amedway uctationstay oaselestay orksaseway\n"
          ]
        }
      ],
      "source": [
        "best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n",
        "best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n",
        "best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n",
        "\n",
        "TEST_SENTENCE = \"flower frame fluctuation forseeable forsake\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Attention mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive attention\n",
        "\n",
        "In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n",
        "            keys\n",
        "        )\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "\n",
        "        context = torch.bmm(attention_weights.transpose(2, 1), values)\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN + additive attention\n",
        "\n",
        "In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "outputs": [],
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n",
        "        if attention_type == \"additive\":\n",
        "            self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == \"scaled_dot\":\n",
        "            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(\n",
        "                h_prev, annotations, annotations\n",
        "            )  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat(\n",
        "                [embed_current, context.squeeze(1)], dim=1\n",
        "            )  # batch_size x (2*hidden_size)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n",
        "\n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and analysis (with additive attention)\n",
        "\n",
        "Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke6t6rCezpZV",
        "outputId": "ac6f15e3-6ef7-4694-a40b-15d262e5a1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('misinformed', 'isinformedmay')\n",
            "('attracted', 'attractedway')\n",
            "('wholly', 'ollywhay')\n",
            "('sketch', 'etchskay')\n",
            "('final', 'inalfay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 1.982 | Val loss: 1.776 | Gen: etay atay indindinday-ondinday issay indway\n",
            "Epoch:   1 | Train loss: 1.456 | Val loss: 1.569 | Gen: ethay ay oncationday issway oway\n",
            "Epoch:   2 | Train loss: 1.177 | Val loss: 1.404 | Gen: ethethethethethethet ay ontintingday issway orway-inghay\n",
            "Epoch:   3 | Train loss: 0.938 | Val loss: 1.261 | Gen: ehay arway ontiongway isway oringway\n",
            "Epoch:   4 | Train loss: 0.779 | Val loss: 1.246 | Gen: ethay-ethay aiway-iway ontiondidingcay isway oligway-ingway-ingwa\n",
            "Epoch:   5 | Train loss: 0.672 | Val loss: 0.969 | Gen: ehay airway ontinidingcay isway oringway\n",
            "Epoch:   6 | Train loss: 0.489 | Val loss: 0.849 | Gen: ethay-abthay airway ondintingifingway isway oringway\n",
            "Epoch:   7 | Train loss: 0.385 | Val loss: 0.745 | Gen: ethay away-away ondintingcay isway orkingway\n",
            "Epoch:   8 | Train loss: 0.315 | Val loss: 0.857 | Gen: ehtay airway onditingcay isway oringway\n",
            "Epoch:   9 | Train loss: 0.283 | Val loss: 0.723 | Gen: ethay aiway onditingcay isway oringway\n",
            "Epoch:  10 | Train loss: 0.211 | Val loss: 0.690 | Gen: ethay airway onitingcay isway oighay\n",
            "Epoch:  11 | Train loss: 0.180 | Val loss: 0.702 | Gen: etay airway ondinitingcay isway owingway\n",
            "Epoch:  12 | Train loss: 0.186 | Val loss: 0.571 | Gen: ethay airway onditiongcay isway oringway\n",
            "Epoch:  13 | Train loss: 0.165 | Val loss: 0.453 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.099 | Val loss: 0.454 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.091 | Val loss: 0.394 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.063 | Val loss: 0.364 | Gen: etay airway onditionicgcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.048 | Val loss: 0.319 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.037 | Val loss: 0.330 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.031 | Val loss: 0.380 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.042 | Val loss: 0.411 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.033 | Val loss: 0.378 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.032 | Val loss: 0.481 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.068 | Val loss: 0.439 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.072 | Val loss: 0.427 | Gen: ethay airway onditingcay isway oringway\n",
            "Epoch:  25 | Train loss: 0.054 | Val loss: 0.373 | Gen: ethay airway onditingway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.045 | Val loss: 0.383 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.050 | Val loss: 0.485 | Gen: etay airway onditingcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.3193379735806957\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehtay airway onditiongcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,\n",
        "    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNVKbLc0ACj_",
        "outputId": "7834af5d-8706-4e0c-f8f4-0f32e4b6d8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tair beta cat dog eye flower simultaneously superficial extraordinary concentration concatenation architecture artificial intelligence \n",
            "translated:\tairway etay atcay ogday eyeway owerflay imultaneouslysay uperficialway extrarodinay-arinay oncentrationcay oncatenationcay architeway artificialway intelligenceway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"air beta cat dog eye flower simultaneously superficial extraordinary concentration concatenation architecture artificial intelligence\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 4: Implement scaled dot-product attention\n",
        "\n",
        "In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "outputs": [],
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # \n",
        "        batch_size = queries.size(dim = 0)\n",
        "        # expand the size of queries to three dimensions if it has dimension 2\n",
        "        q = self.Q(queries)\n",
        "        if(len(queries.size()) == 2):\n",
        "          q = torch.unsqueeze(q, 1)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = (k @ q.transpose(1,2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = attention_weights.transpose(1,2) @ v\n",
        "        return context, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 5: Implement causal dot-product Attention\n",
        "\n",
        "\n",
        "Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "outputs": [],
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = queries.size(dim = 0)\n",
        "        # expand the size of queries to three dimensions if it has dimension \n",
        "        q = self.Q(queries)\n",
        "        if(len(queries.size()) == 2):\n",
        "          q = torch.unsqueeze(q, 1)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = (k @ q.transpose(1, 2)) * self.scaling_factor\n",
        "        mask = torch.tril(torch.ones(unnormalized_attention.shape), -1) * self.neg_inf\n",
        "        mask = mask.to(unnormalized_attention.device)\n",
        "        mask = mask + torch.triu(unnormalized_attention)\n",
        "        unnormalized_attention += mask\n",
        "        attention_weights = self.softmax(mask)\n",
        "        context = attention_weights.transpose(1, 2) @ v\n",
        "        return context, attention_weights\n",
        "\n",
        "        # mask = torch.triu(unnormalized_attention)\n",
        "        # mask[mask == 0] = self.neg_inf\n",
        "        # attention_weights = self.softmax(mask)\n",
        "        # above code shows parts in upper triangle with zero can also be masked, potential risk in training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkjHbtvT6Qxs"
      },
      "source": [
        "## Step 6: Attention encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yKGNqUaX6RLO"
      },
      "outputs": [],
      "source": [
        "class AttentionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(AttentionEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "               \n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder scaled dot attention.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        annotations = encoded\n",
        "        new_annotations, self_attention_weights = self.self_attention(\n",
        "            annotations, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_annotations = annotations + new_annotations\n",
        "        new_annotations = self.attention_mlp(residual_annotations)\n",
        "        annotations = residual_annotations + new_annotations\n",
        "\n",
        "        return annotations, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vDUvtOee7cMy"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = CausalScaledDotAttention(\n",
        "                                hidden_size=hidden_size,\n",
        "                                )\n",
        "                \n",
        "        self.decoder_attention = ScaledDotAttention(\n",
        "                                  hidden_size=hidden_size,\n",
        "                                  )\n",
        "                \n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "                \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        new_contexts, self_attention_weights = self.self_attention(\n",
        "            contexts, contexts, contexts\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = contexts + new_contexts\n",
        "        new_contexts, encoder_attention_weights = self.decoder_attention(\n",
        "            residual_contexts, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = residual_contexts + new_contexts\n",
        "        new_contexts = self.attention_mlp(residual_contexts)\n",
        "        contexts = residual_contexts + new_contexts\n",
        "\n",
        "        encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "        self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7gJLw5t_rnW"
      },
      "source": [
        "## Step 7: Training and analysis (single scaled dot-product attention block)\n",
        "\n",
        "Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7MOkZonC8T3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd08400-2450-453f-bb7d-015576820197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: attention                              \n",
            "                           decoder_type: attention                              \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('misinformed', 'isinformedmay')\n",
            "('attracted', 'attractedway')\n",
            "('wholly', 'ollywhay')\n",
            "('sketch', 'etchskay')\n",
            "('final', 'inalfay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.940 | Val loss: 2.405 | Gen: eay y eay iinay eay \n",
            "Epoch:   1 | Train loss: 2.247 | Val loss: 2.160 | Gen: ecay inay inay inay oinay\n",
            "Epoch:   2 | Train loss: 2.054 | Val loss: 2.040 | Gen: ehay inay inay isay inay\n",
            "Epoch:   3 | Train loss: 1.932 | Val loss: 1.956 | Gen: ehay inay ionconconconcay isay ingay\n",
            "Epoch:   4 | Train loss: 1.840 | Val loss: 1.900 | Gen: ehay inay ioncay isay ingay\n",
            "Epoch:   5 | Train loss: 1.770 | Val loss: 1.850 | Gen: ehay inay ioncay isay ingay\n",
            "Epoch:   6 | Train loss: 1.712 | Val loss: 1.798 | Gen: ehay iray indndndndndndndndndn isay ingtingtay\n",
            "Epoch:   7 | Train loss: 1.660 | Val loss: 1.757 | Gen: ehay iray indndndndndndndndndn isay ingray\n",
            "Epoch:   8 | Train loss: 1.616 | Val loss: 1.719 | Gen: ehay iray incay isay ingray\n",
            "Epoch:   9 | Train loss: 1.576 | Val loss: 1.692 | Gen: ehay iray incay isay ingray\n",
            "Epoch:  10 | Train loss: 1.542 | Val loss: 1.663 | Gen: ehay iray incay isay ingray\n",
            "Epoch:  11 | Train loss: 1.510 | Val loss: 1.640 | Gen: ehay iray ingway isay ingray\n",
            "Epoch:  12 | Train loss: 1.481 | Val loss: 1.618 | Gen: ehay iray ingway isay ingray\n",
            "Epoch:  13 | Train loss: 1.453 | Val loss: 1.600 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  14 | Train loss: 1.428 | Val loss: 1.580 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  15 | Train loss: 1.404 | Val loss: 1.565 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  16 | Train loss: 1.381 | Val loss: 1.550 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  17 | Train loss: 1.359 | Val loss: 1.531 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  18 | Train loss: 1.337 | Val loss: 1.512 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  19 | Train loss: 1.318 | Val loss: 1.496 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  20 | Train loss: 1.298 | Val loss: 1.484 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  21 | Train loss: 1.278 | Val loss: 1.469 | Gen: ethay iray ingway isay ingray\n",
            "Epoch:  22 | Train loss: 1.261 | Val loss: 1.460 | Gen: ethay iray ingway isay orkingray\n",
            "Epoch:  23 | Train loss: 1.243 | Val loss: 1.446 | Gen: ethay iray ingingway isay orkingray\n",
            "Epoch:  24 | Train loss: 1.227 | Val loss: 1.436 | Gen: ethay iray ingingway isay orkingray\n",
            "Epoch:  25 | Train loss: 1.212 | Val loss: 1.425 | Gen: ethay iray ingingway isay orkingray\n",
            "Epoch:  26 | Train loss: 1.197 | Val loss: 1.414 | Gen: ethay iray ingingway isay orkingray\n",
            "Epoch:  27 | Train loss: 1.183 | Val loss: 1.403 | Gen: ethay iray ingingway isay orkingray\n",
            "Epoch:  28 | Train loss: 1.171 | Val loss: 1.392 | Gen: ethay ay ingingway isay orkingray\n",
            "Epoch:  29 | Train loss: 1.155 | Val loss: 1.381 | Gen: ethay ay ingingway isay orkingray\n",
            "Epoch:  30 | Train loss: 1.145 | Val loss: 1.374 | Gen: ethay ay ingiongway isway orkingay\n",
            "Epoch:  31 | Train loss: 1.129 | Val loss: 1.368 | Gen: ethay ay ingingway isway orkingay\n",
            "Epoch:  32 | Train loss: 1.121 | Val loss: 1.359 | Gen: ethay ay ingiongway isway orkingay\n",
            "Epoch:  33 | Train loss: 1.102 | Val loss: 1.341 | Gen: ethay ay ingingway isway orkingway\n",
            "Epoch:  34 | Train loss: 1.090 | Val loss: 1.332 | Gen: ethay ay ingingway isway orkingway\n",
            "Epoch:  35 | Train loss: 1.075 | Val loss: 1.314 | Gen: ethay ay ingingway isway orkingway\n",
            "Epoch:  36 | Train loss: 1.067 | Val loss: 1.305 | Gen: ethay ay ingingway isway orkingway\n",
            "Epoch:  37 | Train loss: 1.051 | Val loss: 1.293 | Gen: ethay ay ingfiongway isway orkingway\n",
            "Epoch:  38 | Train loss: 1.045 | Val loss: 1.288 | Gen: ethay ay ingfiongway isway orkingway\n",
            "Epoch:  39 | Train loss: 1.031 | Val loss: 1.274 | Gen: ethay ay ingfiongway isway orkingway\n",
            "Epoch:  40 | Train loss: 1.026 | Val loss: 1.269 | Gen: ethay ay ingfiongway isway orkingway\n",
            "Epoch:  41 | Train loss: 1.013 | Val loss: 1.259 | Gen: ethay ay ingfiongway isway orkingway\n",
            "Epoch:  42 | Train loss: 1.008 | Val loss: 1.253 | Gen: ethay ay ingfifingway isway orkingway\n",
            "Epoch:  43 | Train loss: 0.995 | Val loss: 1.245 | Gen: ethay arirway ingfifingway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.989 | Val loss: 1.242 | Gen: ethay arirway ingfifingway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.979 | Val loss: 1.235 | Gen: ethay arirway ingfifingway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.973 | Val loss: 1.232 | Gen: ethay arirway ingfifingway isway orkingway\n",
            "Epoch:  47 | Train loss: 0.967 | Val loss: 1.226 | Gen: ethay arirway ingfingway isway orkingway\n",
            "Epoch:  48 | Train loss: 0.959 | Val loss: 1.221 | Gen: ethay arirway ingfifingnday isway orkingway\n",
            "Epoch:  49 | Train loss: 0.952 | Val loss: 1.218 | Gen: ethay arirway ingfingway isway orkingway\n",
            "Epoch:  50 | Train loss: 0.945 | Val loss: 1.213 | Gen: ethay arirway ingfinday isway orkingway\n",
            "Epoch:  51 | Train loss: 0.939 | Val loss: 1.209 | Gen: ethay arirway ingfinday isway owkway\n",
            "Epoch:  52 | Train loss: 0.939 | Val loss: 1.206 | Gen: ethay arirway ingfinday isway owkway\n",
            "Epoch:  53 | Train loss: 0.926 | Val loss: 1.201 | Gen: ethay arirway ingfinday isway owkway\n",
            "Epoch:  54 | Train loss: 0.919 | Val loss: 1.199 | Gen: ethay arirway ingfinday isway owkway\n",
            "Epoch:  55 | Train loss: 0.917 | Val loss: 1.197 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  56 | Train loss: 0.911 | Val loss: 1.194 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  57 | Train loss: 0.903 | Val loss: 1.189 | Gen: ethay arirway ingfinday isway owkway\n",
            "Epoch:  58 | Train loss: 0.900 | Val loss: 1.190 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  59 | Train loss: 0.896 | Val loss: 1.187 | Gen: ethay ariray onidingway isway owkway\n",
            "Epoch:  60 | Train loss: 0.887 | Val loss: 1.184 | Gen: ethay ariray ingfinday isway owkway\n",
            "Epoch:  61 | Train loss: 0.883 | Val loss: 1.187 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  62 | Train loss: 0.878 | Val loss: 1.183 | Gen: ethay ariray onidingway isway owkway\n",
            "Epoch:  63 | Train loss: 0.871 | Val loss: 1.180 | Gen: ethay ariray ingfinday isway owkway\n",
            "Epoch:  64 | Train loss: 0.868 | Val loss: 1.180 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  65 | Train loss: 0.863 | Val loss: 1.174 | Gen: ethay ariray onidingway isway owkway\n",
            "Epoch:  66 | Train loss: 0.856 | Val loss: 1.171 | Gen: ethay arirway ingfinday isway owkway\n",
            "Epoch:  67 | Train loss: 0.853 | Val loss: 1.170 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  68 | Train loss: 0.850 | Val loss: 1.165 | Gen: ethay ariray onidingway isway owkway\n",
            "Epoch:  69 | Train loss: 0.844 | Val loss: 1.162 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  70 | Train loss: 0.842 | Val loss: 1.162 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  71 | Train loss: 0.837 | Val loss: 1.157 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  72 | Train loss: 0.831 | Val loss: 1.156 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  73 | Train loss: 0.828 | Val loss: 1.153 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  74 | Train loss: 0.826 | Val loss: 1.151 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  75 | Train loss: 0.821 | Val loss: 1.149 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  76 | Train loss: 0.818 | Val loss: 1.147 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  77 | Train loss: 0.817 | Val loss: 1.147 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  78 | Train loss: 0.813 | Val loss: 1.142 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  79 | Train loss: 0.809 | Val loss: 1.141 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  80 | Train loss: 0.808 | Val loss: 1.142 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  81 | Train loss: 0.803 | Val loss: 1.137 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  82 | Train loss: 0.799 | Val loss: 1.138 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  83 | Train loss: 0.797 | Val loss: 1.137 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  84 | Train loss: 0.795 | Val loss: 1.134 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  85 | Train loss: 0.791 | Val loss: 1.134 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  86 | Train loss: 0.789 | Val loss: 1.133 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  87 | Train loss: 0.786 | Val loss: 1.131 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  88 | Train loss: 0.784 | Val loss: 1.129 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  89 | Train loss: 0.781 | Val loss: 1.128 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  90 | Train loss: 0.779 | Val loss: 1.127 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  91 | Train loss: 0.776 | Val loss: 1.125 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  92 | Train loss: 0.773 | Val loss: 1.123 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  93 | Train loss: 0.772 | Val loss: 1.122 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  94 | Train loss: 0.769 | Val loss: 1.120 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  95 | Train loss: 0.767 | Val loss: 1.119 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  96 | Train loss: 0.765 | Val loss: 1.117 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  97 | Train loss: 0.763 | Val loss: 1.116 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  98 | Train loss: 0.761 | Val loss: 1.114 | Gen: ethay arirway onidingway isway owkway\n",
            "Epoch:  99 | Train loss: 0.758 | Val loss: 1.114 | Gen: ethay arirway onidingway isway owkway\n",
            "Obtained lowest validation loss of: 1.1138452254235744\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arirway onidingway isway owkway\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1024)\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "attention_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"attention\",\n",
        "    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n",
        "}\n",
        "attention_args_s.update(args_dict)\n",
        "print_opts(attention_args_s)\n",
        "\n",
        "attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 8: Transformer encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer.\n",
        "        return annotations, None\n",
        "        # return annotations, None, None\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                CausalScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.encoder_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
        "                contexts, contexts, contexts\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
        "                residual_contexts, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 9: Training and analysis (with scaled dot-product attention)\n",
        "\n",
        "Now we will train a (simplified) transformer encoder-decoder model.\n",
        "\n",
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ac9065-24c8-447d-d572-820f779e0259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 4                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('misinformed', 'isinformedmay')\n",
            "('attracted', 'attractedway')\n",
            "('wholly', 'ollywhay')\n",
            "('sketch', 'etchskay')\n",
            "('final', 'inalfay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.928 | Val loss: 2.311 | Gen: ayayayyayay ay ieliay ay icay\n",
            "Epoch:   1 | Train loss: 2.009 | Val loss: 1.962 | Gen: aeay ay inonay isway onay\n",
            "Epoch:   2 | Train loss: 1.776 | Val loss: 1.782 | Gen: eay ay ononay isway ongay\n",
            "Epoch:   3 | Train loss: 1.621 | Val loss: 1.778 | Gen: eay ay inonmnay isway ingcay\n",
            "Epoch:   4 | Train loss: 1.521 | Val loss: 1.939 | Gen: eteay arway ionmy isway ongcay\n",
            "Epoch:   5 | Train loss: 1.447 | Val loss: 1.631 | Gen: eay away ondinday isway ongray\n",
            "Epoch:   6 | Train loss: 1.342 | Val loss: 1.627 | Gen: eteay arway indintinday isway ongray\n",
            "Epoch:   7 | Train loss: 1.264 | Val loss: 1.542 | Gen: etetay arway ondintingy isway ongray\n",
            "Epoch:   8 | Train loss: 1.204 | Val loss: 1.503 | Gen: eteteteay areray ondintinday isway ongray\n",
            "Epoch:   9 | Train loss: 1.146 | Val loss: 1.717 | Gen: eteyyyyyyyyyyyyyyyyy irway ondintiontiontindngy isway ongray\n",
            "Epoch:  10 | Train loss: 1.135 | Val loss: 1.524 | Gen: etehay areray ondintinday isway ongray\n",
            "Epoch:  11 | Train loss: 1.091 | Val loss: 1.552 | Gen: etehay arway odintinday isway ongray\n",
            "Epoch:  12 | Train loss: 1.098 | Val loss: 1.466 | Gen: ehay arway indintingiontintingn isway orway\n",
            "Epoch:  13 | Train loss: 1.018 | Val loss: 1.442 | Gen: ehey arway indintingay isway orway\n",
            "Epoch:  14 | Train loss: 0.952 | Val loss: 1.335 | Gen: ehay arway odintiongay isway orway\n",
            "Epoch:  15 | Train loss: 0.892 | Val loss: 1.407 | Gen: ehey arway odintiongay isway orway\n",
            "Epoch:  16 | Train loss: 0.852 | Val loss: 1.317 | Gen: ehay arway iscintiongiongiongay isway orway\n",
            "Epoch:  17 | Train loss: 0.813 | Val loss: 1.342 | Gen: ehay arway icdintiontiongiongwa isway orway\n",
            "Epoch:  18 | Train loss: 0.778 | Val loss: 1.299 | Gen: ehay arway indisatiniontiongway isway orway\n",
            "Epoch:  19 | Train loss: 0.750 | Val loss: 1.321 | Gen: eaythay arway indicay isway orway\n",
            "Epoch:  20 | Train loss: 0.734 | Val loss: 1.264 | Gen: ehay arway indicatingiongingway isway orway\n",
            "Epoch:  21 | Train loss: 0.706 | Val loss: 1.237 | Gen: etaythay arway indciigicay isway owayngway\n",
            "Epoch:  22 | Train loss: 0.677 | Val loss: 1.163 | Gen: ehay arway indcitioniontiongway isway orway-ingway\n",
            "Epoch:  23 | Train loss: 0.629 | Val loss: 1.152 | Gen: ethay arway indcationgiontiongca isway orway-ingay\n",
            "Epoch:  24 | Train loss: 0.601 | Val loss: 1.058 | Gen: ethay arway ondidintiongcay isway orway-igway\n",
            "Epoch:  25 | Train loss: 0.558 | Val loss: 1.073 | Gen: ethay arway onditingday isway orway-igway\n",
            "Epoch:  26 | Train loss: 0.534 | Val loss: 1.052 | Gen: ethay arway ondigintiongcay isway orrway-igway\n",
            "Epoch:  27 | Train loss: 0.511 | Val loss: 1.068 | Gen: ethay arway indintiongiongcay isway orringway\n",
            "Epoch:  28 | Train loss: 0.522 | Val loss: 1.062 | Gen: eththay arway onditingcay isway orwway\n",
            "Epoch:  29 | Train loss: 0.581 | Val loss: 1.212 | Gen: ethay arway ondinitiongciongway isway orngrway\n",
            "Epoch:  30 | Train loss: 0.604 | Val loss: 1.048 | Gen: ethay arway indintiongay isway orwngway\n",
            "Epoch:  31 | Train loss: 0.505 | Val loss: 0.938 | Gen: ethay arway indintiongcay isway orwwngway\n",
            "Epoch:  32 | Train loss: 0.459 | Val loss: 0.966 | Gen: eththay arway onditingcay isway orwngray\n",
            "Epoch:  33 | Train loss: 0.433 | Val loss: 0.951 | Gen: ethay arway ondinintiongcay isway orwngway\n",
            "Epoch:  34 | Train loss: 0.405 | Val loss: 0.909 | Gen: ethay arway onditingcay isway orwngway\n",
            "Epoch:  35 | Train loss: 0.380 | Val loss: 0.896 | Gen: ethay arirway onditingcay isway orwngway\n",
            "Epoch:  36 | Train loss: 0.368 | Val loss: 0.934 | Gen: ethay ariway onditingcay isway orwngway\n",
            "Epoch:  37 | Train loss: 0.357 | Val loss: 0.879 | Gen: ethay ariway onditingcay isway orwngway\n",
            "Epoch:  38 | Train loss: 0.345 | Val loss: 0.922 | Gen: ethay ariway onditingciongcay isway orwngway\n",
            "Epoch:  39 | Train loss: 0.331 | Val loss: 0.913 | Gen: eththay arway ondinicationgciongwa isway orwngway\n",
            "Epoch:  40 | Train loss: 0.343 | Val loss: 1.084 | Gen: ethay arway ondicintiongcicay isway orwngway\n",
            "Epoch:  41 | Train loss: 0.435 | Val loss: 1.149 | Gen: eththay irway ondinintiongciongway isway orwngngway\n",
            "Epoch:  42 | Train loss: 0.456 | Val loss: 1.002 | Gen: ethay ariway ondidintciongcay isway orrngway\n",
            "Epoch:  43 | Train loss: 0.420 | Val loss: 1.033 | Gen: eththay arirway ondintininicationgci iswisiwinway orngringray\n",
            "Epoch:  44 | Train loss: 0.413 | Val loss: 0.957 | Gen: eththay arwarwarway ondidintioniongcay isway orwngway\n",
            "Epoch:  45 | Train loss: 0.338 | Val loss: 0.927 | Gen: ethay ariway ondidintiongcay isway orwngway\n",
            "Epoch:  46 | Train loss: 0.301 | Val loss: 0.849 | Gen: ethay ariway onditiniongciotingca isway orwngway\n",
            "Epoch:  47 | Train loss: 0.273 | Val loss: 0.798 | Gen: eththay ariway onditingciongcay isway orwngway\n",
            "Epoch:  48 | Train loss: 0.255 | Val loss: 0.785 | Gen: ethay ariway onditingciongcay isway orwngway\n",
            "Epoch:  49 | Train loss: 0.242 | Val loss: 0.781 | Gen: ethay ariway onditingciongcay isway orwngway\n",
            "Epoch:  50 | Train loss: 0.232 | Val loss: 0.779 | Gen: ethay ariway onditingciongcay isway orwngway\n",
            "Epoch:  51 | Train loss: 0.223 | Val loss: 0.777 | Gen: ethay ariway onditingciongcay isway orwngway\n",
            "Epoch:  52 | Train loss: 0.215 | Val loss: 0.778 | Gen: ethay ariway onditingciongcay isway orwngway\n",
            "Epoch:  53 | Train loss: 0.207 | Val loss: 0.772 | Gen: ethay ariway onditincingicay isway orwngway\n",
            "Epoch:  54 | Train loss: 0.200 | Val loss: 0.787 | Gen: ethay ariway onditingciongcay isway orwngway\n",
            "Epoch:  55 | Train loss: 0.194 | Val loss: 0.777 | Gen: ethay ariway onditincingiay isway orwngwngway\n",
            "Epoch:  56 | Train loss: 0.189 | Val loss: 0.796 | Gen: ethay arirway onditingciongcay isway orwngway\n",
            "Epoch:  57 | Train loss: 0.215 | Val loss: 1.052 | Gen: ehthay awarway onditinincationcifif isway orwngngway\n",
            "Epoch:  58 | Train loss: 0.315 | Val loss: 0.906 | Gen: etethay arway ondiciiotiongcigcay isway orwngway\n",
            "Epoch:  59 | Train loss: 0.281 | Val loss: 0.863 | Gen: ethay arway onditingcinicicay isway orwngwngway\n",
            "Epoch:  60 | Train loss: 0.229 | Val loss: 0.757 | Gen: ethay arway onditincinfay isway orwngwngway\n",
            "Epoch:  61 | Train loss: 0.187 | Val loss: 0.768 | Gen: ethay awarway onditincingway isway orwngway\n",
            "Epoch:  62 | Train loss: 0.171 | Val loss: 0.755 | Gen: ethay arway onditincingway isway orwngway\n",
            "Epoch:  63 | Train loss: 0.162 | Val loss: 0.761 | Gen: ethay awarway onditincingway isway orwngway\n",
            "Epoch:  64 | Train loss: 0.154 | Val loss: 0.763 | Gen: ethay awarway onditincingway isway orwngway\n",
            "Epoch:  65 | Train loss: 0.149 | Val loss: 0.766 | Gen: ethay awarway onditincingway isway orwngwngway\n",
            "Epoch:  66 | Train loss: 0.143 | Val loss: 0.769 | Gen: ethay awarway onditincingway isway orkingway\n",
            "Epoch:  67 | Train loss: 0.138 | Val loss: 0.775 | Gen: ethay awarway onditincinginay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.133 | Val loss: 0.777 | Gen: ethay arway onditincinginay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.129 | Val loss: 0.779 | Gen: ethay arway onditincinginay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.124 | Val loss: 0.782 | Gen: ethay arway onditincinginay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.120 | Val loss: 0.785 | Gen: ethay arway onditincinginay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.116 | Val loss: 0.790 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.112 | Val loss: 0.793 | Gen: ethay arway onditiningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.109 | Val loss: 0.804 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.105 | Val loss: 0.797 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.101 | Val loss: 0.838 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.099 | Val loss: 0.824 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.095 | Val loss: 0.833 | Gen: ethay arway onditiningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.152 | Val loss: 1.482 | Gen: ethay away onditinghay isway orkikingkkkkkkay\n",
            "Epoch:  80 | Train loss: 0.393 | Val loss: 1.089 | Gen: ehay arway ondicioniotiongc isway orkingwnray\n",
            "Epoch:  81 | Train loss: 0.295 | Val loss: 0.824 | Gen: ethay arway onditingingchay isway orwngwngway\n",
            "Epoch:  82 | Train loss: 0.170 | Val loss: 0.715 | Gen: ethay arirway onditiniday isway orkingway\n",
            "Epoch:  83 | Train loss: 0.125 | Val loss: 0.719 | Gen: ethay arway onditiningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.106 | Val loss: 0.723 | Gen: ethay arway onditiningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.097 | Val loss: 0.728 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.091 | Val loss: 0.737 | Gen: ethay arway onditiningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.086 | Val loss: 0.746 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.082 | Val loss: 0.754 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.079 | Val loss: 0.764 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.076 | Val loss: 0.771 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.074 | Val loss: 0.779 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.071 | Val loss: 0.786 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.069 | Val loss: 0.793 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.067 | Val loss: 0.801 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.065 | Val loss: 0.808 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.063 | Val loss: 0.817 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.061 | Val loss: 0.823 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.059 | Val loss: 0.832 | Gen: ethay arway onditinincay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.057 | Val loss: 0.839 | Gen: ethay arway onditinincay isway orkingway\n",
            "Obtained lowest validation loss of: 0.715471369586885\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arway onditinincay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1024)\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 4,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04308a4a-1a7c-45c8-da58-7f8a32730fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arway onditinincay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542a5982-cd51-45be-d8e0-e626e6aa8161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('tenderest', 'enderesttay')\n",
            "('oracle', 'oracleway')\n",
            "('shave', 'aveshay')\n",
            "('corruption', 'orruptioncay')\n",
            "('formally', 'ormallyfay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.737 | Val loss: 2.502 | Gen: e------------------- i-ray-ay-ray-ay-ray- onsitndndndttttttttt i-ri------y---y oay-ay-e----\n",
            "Epoch:   1 | Train loss: 2.159 | Val loss: 2.156 | Gen: ettay-ay-ay-ay-tay-t iray ontay-ntintay-tititt iray onay-ay-eday-ay\n",
            "Epoch:   2 | Train loss: 1.930 | Val loss: 2.034 | Gen: eway-ay-ay-ay-ay-ay- irway-ay-ay ontay-ontintintinttt iray onway-ay-ay-ay-ay-ay\n",
            "Epoch:   3 | Train loss: 1.785 | Val loss: 1.923 | Gen: eway-ay-ay-ay rway onway-ongay-onway-on issay onway-ay-ay-ay\n",
            "Epoch:   4 | Train loss: 1.664 | Val loss: 1.862 | Gen: eway-ay-ay-ay-ay-ay- raway-ray ondingdway-ongdway-o isssay onway-onway-ay-nway-\n",
            "Epoch:   5 | Train loss: 1.578 | Val loss: 1.788 | Gen: eway-ay arway ondindndnday-ondway- isay onggway-inway\n",
            "Epoch:   6 | Train loss: 1.493 | Val loss: 1.811 | Gen: eway-ay-ay-ay-ay-ay- arway ondngongngay-ongngng iway ongggway-inway-nway\n",
            "Epoch:   7 | Train loss: 1.437 | Val loss: 1.700 | Gen: eway arway ontay-ondingngngngnw say onggway\n",
            "Epoch:   8 | Train loss: 1.366 | Val loss: 1.695 | Gen: ethay arway ongay-ongngnway iway ongway-onway\n",
            "Epoch:   9 | Train loss: 1.317 | Val loss: 1.678 | Gen: eway arway ongay-ongngnay say ongway-inway\n",
            "Epoch:  10 | Train loss: 1.253 | Val loss: 1.651 | Gen: ethay arway ongay-ingay-inwinway isay ongway-inway\n",
            "Epoch:  11 | Train loss: 1.210 | Val loss: 1.589 | Gen: ethay arway ongay-ingngngay isay onggway-inway\n",
            "Epoch:  12 | Train loss: 1.152 | Val loss: 1.584 | Gen: ethay arrray ongay-ingay isay oronghay-inway\n",
            "Epoch:  13 | Train loss: 1.121 | Val loss: 1.465 | Gen: ethay arrway ongingay-iongngway isay ongway-ingay\n",
            "Epoch:  14 | Train loss: 1.071 | Val loss: 1.431 | Gen: ethay arrray ongingay-ongay isway ororingay\n",
            "Epoch:  15 | Train loss: 1.040 | Val loss: 1.405 | Gen: ethay ariray ongingay-iongngway isway ougglay\n",
            "Epoch:  16 | Train loss: 1.012 | Val loss: 1.483 | Gen: ethehay ariray ongingay-ongay-ongwa isway orkgkay-ingay\n",
            "Epoch:  17 | Train loss: 1.000 | Val loss: 1.377 | Gen: ethay aray ongingationgay isway ongay-ingway\n",
            "Epoch:  18 | Train loss: 0.959 | Val loss: 1.423 | Gen: ethhhay ariray ongingdatgay isay orkgkay-ingay\n",
            "Epoch:  19 | Train loss: 0.942 | Val loss: 1.299 | Gen: ethay arway ongingationgday isaway orkgay-igway\n",
            "Epoch:  20 | Train loss: 0.902 | Val loss: 1.409 | Gen: ethehay arrway ongingatingdngway isssay orkgkay-ingay\n",
            "Epoch:  21 | Train loss: 0.897 | Val loss: 1.276 | Gen: hthay arrway ondingdaytingngday isay orkgkay-ingay\n",
            "Epoch:  22 | Train loss: 0.856 | Val loss: 1.353 | Gen: ethhhhhay arrray ongindangay-ingway issay orgkbay-ingay\n",
            "Epoch:  23 | Train loss: 0.846 | Val loss: 1.210 | Gen: ethhay aray ongingay-ongngngway isay orkgkay-ingay\n",
            "Epoch:  24 | Train loss: 0.810 | Val loss: 1.333 | Gen: ethhhhay array ongitingay-ingway isssay orkgkngay-ingway\n",
            "Epoch:  25 | Train loss: 0.799 | Val loss: 1.278 | Gen: ethay awiray ondingayyyingngay isay orkgay-igay\n",
            "Epoch:  26 | Train loss: 0.782 | Val loss: 1.263 | Gen: ethhhay ariray ondingiongay issay orkgkingay\n",
            "Epoch:  27 | Train loss: 0.754 | Val loss: 1.115 | Gen: ethay arway ondingayiongnggway isay orkgay-ingay\n",
            "Epoch:  28 | Train loss: 0.729 | Val loss: 1.156 | Gen: ethethay ariway ongindayiongngway issay orkgkay-ingway\n",
            "Epoch:  29 | Train loss: 0.718 | Val loss: 1.082 | Gen: ethay arway ondingdayingngay isway orkgay-igway\n",
            "Epoch:  30 | Train loss: 0.688 | Val loss: 1.113 | Gen: ethehay arrway ondingiongingay isay orkgkay-ingway\n",
            "Epoch:  31 | Train loss: 0.676 | Val loss: 1.076 | Gen: ethay arway ondingayiongngay isay orkgkay-ingay\n",
            "Epoch:  32 | Train loss: 0.655 | Val loss: 1.088 | Gen: ethehay arrway ondingiongingay isay orkgkingay\n",
            "Epoch:  33 | Train loss: 0.638 | Val loss: 1.077 | Gen: ethay arway ondingayiongngay isway orkgkingay\n",
            "Epoch:  34 | Train loss: 0.619 | Val loss: 1.075 | Gen: ethehay arnway ondingiongdinggay isay orkgkingay\n",
            "Epoch:  35 | Train loss: 0.601 | Val loss: 1.053 | Gen: ethay array ondingionggngngay isway orkgkingay\n",
            "Epoch:  36 | Train loss: 0.589 | Val loss: 1.096 | Gen: ethay awiray ondingiongenggway isay orkgkingay\n",
            "Epoch:  37 | Train loss: 0.588 | Val loss: 0.981 | Gen: ethay arway ondingiongngnggway isway orknkay-ongay\n",
            "Epoch:  38 | Train loss: 0.563 | Val loss: 1.064 | Gen: ethhay arrway ondingiongdinggay isay orknkiggay\n",
            "Epoch:  39 | Train loss: 0.550 | Val loss: 0.930 | Gen: ethay arway ondingiongngngay isway orknkingay\n",
            "Epoch:  40 | Train loss: 0.525 | Val loss: 0.948 | Gen: ethhay arrway ondingionggnggway isway orknkiggay\n",
            "Epoch:  41 | Train loss: 0.513 | Val loss: 0.916 | Gen: ethay arrway ondingiongngngay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.505 | Val loss: 1.001 | Gen: ethhay airray ondingiongngngay isay orknkingay\n",
            "Epoch:  43 | Train loss: 0.505 | Val loss: 0.935 | Gen: ethay arrway ondingiongngngay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.504 | Val loss: 1.051 | Gen: ethay awirway ondindayiongngway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.511 | Val loss: 0.944 | Gen: ethay awiray ondingingngngngway isway orkingkay\n",
            "Epoch:  46 | Train loss: 0.500 | Val loss: 1.075 | Gen: ethhhay airrway ondingioray isisay orknkigway\n",
            "Epoch:  47 | Train loss: 0.539 | Val loss: 0.996 | Gen: ethway away ondingingngngnday isway orkingngway\n",
            "Epoch:  48 | Train loss: 0.512 | Val loss: 0.883 | Gen: ethhay arrway ondingioray isway orkingway\n",
            "Epoch:  49 | Train loss: 0.470 | Val loss: 0.878 | Gen: ethay arway ondingdiongngngway isway orkingway\n",
            "Epoch:  50 | Train loss: 0.439 | Val loss: 0.827 | Gen: ethhay arrway onditingongingway isway orkingway\n",
            "Epoch:  51 | Train loss: 0.416 | Val loss: 0.813 | Gen: ethay ariway ondindiongongngway isway orkingway\n",
            "Epoch:  52 | Train loss: 0.409 | Val loss: 0.844 | Gen: ethhay arrway onditingongngnay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.406 | Val loss: 0.783 | Gen: ethay ariway onditingongngnay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.395 | Val loss: 0.836 | Gen: ethhay arrway ondindiongngngway isway orkingway\n",
            "Epoch:  55 | Train loss: 0.387 | Val loss: 0.782 | Gen: ethay ariway onditingplingngway isway orkingway\n",
            "Epoch:  56 | Train loss: 0.376 | Val loss: 0.809 | Gen: ethhay arrway onditingongngway isway orkingway\n",
            "Epoch:  57 | Train loss: 0.369 | Val loss: 0.764 | Gen: ethay ariway onditingongngway isway orkingway\n",
            "Epoch:  58 | Train loss: 0.360 | Val loss: 0.796 | Gen: ethhay ariway onditingongngway issway orkingway\n",
            "Epoch:  59 | Train loss: 0.355 | Val loss: 0.748 | Gen: ethay ariway onditingongngway isway orkingway\n",
            "Epoch:  60 | Train loss: 0.346 | Val loss: 0.797 | Gen: ethhay airway onditiongngngway issway orkingway\n",
            "Epoch:  61 | Train loss: 0.346 | Val loss: 0.737 | Gen: ethway airway onditingongngway isway orkingway\n",
            "Epoch:  62 | Train loss: 0.335 | Val loss: 0.762 | Gen: ethhay airway onditiongingngay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.329 | Val loss: 0.736 | Gen: ethway airway onditingongngway isway orkingway\n",
            "Epoch:  64 | Train loss: 0.320 | Val loss: 0.766 | Gen: ethhay airway onditiongngngway issway orkingway\n",
            "Epoch:  65 | Train loss: 0.316 | Val loss: 0.725 | Gen: ethway airway onditingongngway isway orkingway\n",
            "Epoch:  66 | Train loss: 0.308 | Val loss: 0.749 | Gen: ethhay airway onditiongngngway issway orkingway\n",
            "Epoch:  67 | Train loss: 0.306 | Val loss: 0.716 | Gen: ethway airway onditingongngway isway orkingway\n",
            "Epoch:  68 | Train loss: 0.301 | Val loss: 0.781 | Gen: ethhay airway onditiongingngay issway orkingway\n",
            "Epoch:  69 | Train loss: 0.303 | Val loss: 0.712 | Gen: ethway airway onditiongingngay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.292 | Val loss: 0.732 | Gen: ethhay airway onditiongngngway issway orkingway\n",
            "Epoch:  71 | Train loss: 0.286 | Val loss: 0.711 | Gen: ethway airway onditingiongngway isway orkingway\n",
            "Epoch:  72 | Train loss: 0.281 | Val loss: 0.732 | Gen: ethhay airway onditiongingngay issway orkingway\n",
            "Epoch:  73 | Train loss: 0.274 | Val loss: 0.680 | Gen: ethway airway onditiongingngay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.267 | Val loss: 0.776 | Gen: ethhay airway onditiongingngay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.275 | Val loss: 0.709 | Gen: ethhay airway onditiongingngay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.270 | Val loss: 0.927 | Gen: ethhay airrway ondidiongingngay issway orkingway\n",
            "Epoch:  77 | Train loss: 0.291 | Val loss: 0.746 | Gen: ethway airway onditiongingay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.263 | Val loss: 0.691 | Gen: ethhay airway onditiongingnay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.250 | Val loss: 0.646 | Gen: ethay airway onditiongingngay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.240 | Val loss: 0.676 | Gen: ethhay airway onditiongingnay issay orkingway\n",
            "Epoch:  81 | Train loss: 0.235 | Val loss: 0.648 | Gen: ethay airway onditiongingnay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.229 | Val loss: 0.692 | Gen: ethhay airway onditiongingnay issway orkingway\n",
            "Epoch:  83 | Train loss: 0.231 | Val loss: 0.643 | Gen: ethay airway onditiongingnay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.225 | Val loss: 0.696 | Gen: ethhay airway onditiongingnay issway orkingway\n",
            "Epoch:  85 | Train loss: 0.234 | Val loss: 0.683 | Gen: ethay airway onditiongingway isway oringbay\n",
            "Epoch:  86 | Train loss: 0.235 | Val loss: 0.703 | Gen: hetway airway onditiongingnay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.221 | Val loss: 0.634 | Gen: ethay airway onditiongingnay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.210 | Val loss: 0.676 | Gen: hetway airway onditiongingnay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.207 | Val loss: 0.644 | Gen: ethay airway onditiongingnay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.201 | Val loss: 0.676 | Gen: ethhay airway onditiongingnay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.200 | Val loss: 0.644 | Gen: ethay airway onditiongingnay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.195 | Val loss: 0.687 | Gen: ethhay airway onditiongingnay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.199 | Val loss: 0.635 | Gen: hetway airway onditiongingnay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.194 | Val loss: 0.699 | Gen: ethhay airway onditiongingnay issway orkingway\n",
            "Epoch:  95 | Train loss: 0.197 | Val loss: 0.629 | Gen: hetway airway onditiongingway isway orkingway\n",
            "Epoch:  96 | Train loss: 0.186 | Val loss: 0.654 | Gen: ethay airway onditiongingnay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.184 | Val loss: 0.627 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Epoch:  98 | Train loss: 0.178 | Val loss: 0.651 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Epoch:  99 | Train loss: 0.175 | Val loss: 0.628 | Gen: ethay airway onditiongingway isway orkingway\n",
            "Obtained lowest validation loss of: 0.6271883726728206\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditiongingway isway orkingway\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1024)\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b246fadf-ae6c-4173-d8a7-5d026beb5e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('misinformed', 'isinformedmay')\n",
            "('attracted', 'attractedway')\n",
            "('wholly', 'ollywhay')\n",
            "('sketch', 'etchskay')\n",
            "('final', 'inalfay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.505 | Val loss: 1.977 | Gen: eay ay-ay-ay-ay-ay ionconcay-way-cay-wa iway oooiway-way-way-way-\n",
            "Epoch:   1 | Train loss: 1.692 | Val loss: 1.726 | Gen: ay ay-ay-ay-ay oionincaioncay iway oray-onay-ongay-ony-\n",
            "Epoch:   2 | Train loss: 1.447 | Val loss: 1.696 | Gen: ihay iay-ay-iay indinday-ondincay isayay oninsay-ay\n",
            "Epoch:   3 | Train loss: 1.281 | Val loss: 1.566 | Gen: ay iay oinindincay-oncay isay oringingingray\n",
            "Epoch:   4 | Train loss: 1.131 | Val loss: 1.358 | Gen: hty arway oncidincay isay orkkkay\n",
            "Epoch:   5 | Train loss: 0.979 | Val loss: 1.280 | Gen: ethay iway ondingingdiongtingdw isway okingray\n",
            "Epoch:   6 | Train loss: 0.872 | Val loss: 1.500 | Gen: ehthay-ay irwaray-iaway ondindindingongongon isway okkkngringnglay\n",
            "Epoch:   7 | Train loss: 0.835 | Val loss: 1.064 | Gen: ethay away ondinay isway okinglingngnglay\n",
            "Epoch:   8 | Train loss: 0.649 | Val loss: 0.973 | Gen: ethay iay ondiningingingcay isway oikingway\n",
            "Epoch:   9 | Train loss: 0.570 | Val loss: 1.021 | Gen: ethay airiay ondindindiongcay isway okikngngray\n",
            "Epoch:  10 | Train loss: 0.517 | Val loss: 0.871 | Gen: ethay iay ondiniongingcay isway orkingingway\n",
            "Epoch:  11 | Train loss: 0.472 | Val loss: 0.870 | Gen: ethay ariay onditiongiongcay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.402 | Val loss: 0.776 | Gen: ethay away ondintincay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.333 | Val loss: 0.775 | Gen: ethay airway ondintingingcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.314 | Val loss: 0.992 | Gen: ehay arrway ondintincay iway orkingway\n",
            "Epoch:  15 | Train loss: 0.332 | Val loss: 0.749 | Gen: ethay aiay onditionciongcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.270 | Val loss: 0.768 | Gen: ethay airway onditincay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.246 | Val loss: 0.881 | Gen: ethay airway onitinincay isay orkingway\n",
            "Epoch:  18 | Train loss: 0.264 | Val loss: 0.802 | Gen: ethay airway onditincincay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.217 | Val loss: 0.791 | Gen: tthay airway onditioniongctcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.198 | Val loss: 0.816 | Gen: ethay airway onditioncingcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.187 | Val loss: 0.540 | Gen: ethay away onditioniongcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.136 | Val loss: 0.585 | Gen: ethay airway onditioniongincay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.117 | Val loss: 0.604 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.103 | Val loss: 0.468 | Gen: ethay iay onditioningcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.070 | Val loss: 0.555 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.072 | Val loss: 0.471 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.065 | Val loss: 0.532 | Gen: ethay aiway onditioningcay isay orkingway\n",
            "Epoch:  28 | Train loss: 0.063 | Val loss: 0.440 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.044 | Val loss: 0.484 | Gen: ethay airway onditioniongcay issay orkingway\n",
            "Epoch:  30 | Train loss: 0.041 | Val loss: 0.458 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.049 | Val loss: 0.598 | Gen: ethay aiway onditiongcincgay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.085 | Val loss: 0.603 | Gen: ethay iay onditioningingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.144 | Val loss: 0.792 | Gen: ethay aiway onointinconcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.241 | Val loss: 0.688 | Gen: ethay airway onditiondingngcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.189 | Val loss: 0.601 | Gen: ethay ariway onditioniongcay issay orkingway\n",
            "Epoch:  36 | Train loss: 0.193 | Val loss: 0.600 | Gen: ethay iayway onditioningcay issway orkngway\n",
            "Epoch:  37 | Train loss: 0.152 | Val loss: 0.413 | Gen: ethay iayway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.088 | Val loss: 0.390 | Gen: ethay iayway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.054 | Val loss: 0.360 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.033 | Val loss: 0.342 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.025 | Val loss: 0.364 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.021 | Val loss: 0.351 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.017 | Val loss: 0.363 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.014 | Val loss: 0.363 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.011 | Val loss: 0.366 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.010 | Val loss: 0.379 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.009 | Val loss: 0.395 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.008 | Val loss: 0.389 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.007 | Val loss: 0.408 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.3420589083340019\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1024)\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41ce745-6e80-4ba0-ecfa-377ad9c18431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('tenderest', 'enderesttay')\n",
            "('oracle', 'oracleway')\n",
            "('shave', 'aveshay')\n",
            "('corruption', 'orruptioncay')\n",
            "('formally', 'ormallyfay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.513 | Val loss: 2.421 | Gen: ey ilway-w oooooooooooonininoni iy oooooot-wlway-wl\n",
            "Epoch:   1 | Train loss: 1.816 | Val loss: 1.878 | Gen: ehty aray-ay-ay oodndnday-ondncay iway ongay-onay\n",
            "Epoch:   2 | Train loss: 1.517 | Val loss: 1.786 | Gen: ehay ayayayay oooooooday isay oryay-ory\n",
            "Epoch:   3 | Train loss: 1.340 | Val loss: 1.584 | Gen: eay-ay arway-ayay ointinginginglay isay ory-ingy\n",
            "Epoch:   4 | Train loss: 1.210 | Val loss: 1.476 | Gen: eay arway ointingingdgay isay orkway\n",
            "Epoch:   5 | Train loss: 1.080 | Val loss: 1.456 | Gen: etay arway-iay oidintindsngrdtay isay orkkngray\n",
            "Epoch:   6 | Train loss: 0.974 | Val loss: 1.400 | Gen: ethay arrway odintingingingdway isway orkway\n",
            "Epoch:   7 | Train loss: 0.878 | Val loss: 1.298 | Gen: etay arway oninticatingngdgay isay okkkngay\n",
            "Epoch:   8 | Train loss: 0.785 | Val loss: 1.118 | Gen: ethay arway ondinatingingncay isway orkingay\n",
            "Epoch:   9 | Train loss: 0.705 | Val loss: 1.211 | Gen: ehay away ondiginay-ingay iway owingway\n",
            "Epoch:  10 | Train loss: 0.703 | Val loss: 1.036 | Gen: ehay arway ondicatingdcay isway orkingray\n",
            "Epoch:  11 | Train loss: 0.595 | Val loss: 1.107 | Gen: ethay awaray onoidinatingincay isway oorkingray\n",
            "Epoch:  12 | Train loss: 0.561 | Val loss: 0.974 | Gen: ehay away ondinincay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.484 | Val loss: 0.866 | Gen: ehthay away ondiciniongigcay isway oorkingway\n",
            "Epoch:  14 | Train loss: 0.439 | Val loss: 0.966 | Gen: ethay away ondinincingincay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.407 | Val loss: 0.774 | Gen: ehthay away onditinincay isway orkinginay\n",
            "Epoch:  16 | Train loss: 0.362 | Val loss: 0.743 | Gen: ehay away ondininingngngcay isway orkingngay\n",
            "Epoch:  17 | Train loss: 0.337 | Val loss: 0.867 | Gen: ethay airway ooodininingincay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.348 | Val loss: 0.746 | Gen: ethay away ondicitingcay isway orkinghray\n",
            "Epoch:  19 | Train loss: 0.304 | Val loss: 0.706 | Gen: ethay away oonditiongincay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.291 | Val loss: 0.740 | Gen: ethay awirway oonidioningicay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.272 | Val loss: 0.664 | Gen: ehthay iway onditioningcay isway orkinghay\n",
            "Epoch:  22 | Train loss: 0.239 | Val loss: 0.706 | Gen: ethay away onditiongcingcay isway orkinghway\n",
            "Epoch:  23 | Train loss: 0.258 | Val loss: 0.677 | Gen: ethay awirway onditioningincay isway orkingbray\n",
            "Epoch:  24 | Train loss: 0.232 | Val loss: 0.766 | Gen: ethay away ondititingingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.237 | Val loss: 0.609 | Gen: ethtay ariway onditioningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.190 | Val loss: 0.491 | Gen: ethay airway onditioningingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.173 | Val loss: 0.474 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.153 | Val loss: 0.446 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.130 | Val loss: 0.394 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.122 | Val loss: 0.386 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.115 | Val loss: 0.464 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.153 | Val loss: 0.415 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.118 | Val loss: 0.407 | Gen: ehtay iarway onditioningcay issway ooringway\n",
            "Epoch:  34 | Train loss: 0.097 | Val loss: 0.333 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.096 | Val loss: 0.560 | Gen: eethay airway oddditinningcay issway ooringway\n",
            "Epoch:  36 | Train loss: 0.126 | Val loss: 0.441 | Gen: ethay airway onditionningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.087 | Val loss: 0.458 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.081 | Val loss: 0.327 | Gen: ethay airway onditioninginay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.064 | Val loss: 0.319 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.058 | Val loss: 0.326 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.058 | Val loss: 0.355 | Gen: ethay iarway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.075 | Val loss: 0.577 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.206 | Val loss: 0.611 | Gen: ehtay airway onditionongingcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.225 | Val loss: 0.448 | Gen: ethay airway onditinongigcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.160 | Val loss: 0.344 | Gen: ethay iarway onditionngingcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.099 | Val loss: 0.362 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.109 | Val loss: 0.295 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.058 | Val loss: 0.256 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.040 | Val loss: 0.234 | Gen: ethay airway onditionngincay isway orkingway\n",
            "Obtained lowest validation loss of: 0.2342876510466544\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionngincay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1024)\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e0a6cb95-8244-4a29-b3d1-0e80fb4d2165"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "save_loss_comparison_by_dataset(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_dataset\",\n",
        ")\n",
        "save_loss_comparison_by_hidden(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_hidden\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TjPTaRB4mpCd",
        "s9IS9B9-yUU5",
        "9DaTdRNuUra7",
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30",
        "AJSafHSAmu_w",
        "9tcpUFKqo2Oi"
      ],
      "name": "“nmt.ipynb”的副本",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}